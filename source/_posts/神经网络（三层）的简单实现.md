---
title: 神经网络（三层）的简单实现
date: 2020-03-21 09:39:16
tags: 神经网络 异或运算
keywords: 神经网络 机器学习 三层 异或运算
categories: AI
description: 学习了MIT AI Course的Neural Network部分，决定花点时间自己使用python从零打造一个属于自己的简单神经网络
summary_img:
---

> 最近观看了 MIT Open Course 的 Neural Nets part，对于现代人工智能发展进程颇有感悟，然而纸上得来终觉浅，再加上一直对 machine learning 有兴趣，所以决定使用 python 实现一个简单的神经网络

## Part I 原理分析

Neural Network，神经网络，本质上是矩阵运算以及权值调整，基本工作流程如下：

<img src="/assets/neuralnets1.png" style="display:block;max-width:50%">

`ps：图中忘了加bias，后面公式里会有补上`

既然已经了解到了工作原理，那么接下来就是我最爱的公式推导环节（bushi）

### 1 FORWARD PROPAGATION (前推)

> FORWARD PROPAGATION 这一步的目的是根据输入值得到输出的值，以此来获取预测值

#### 加权

$$z1 = (x_1,x_2)\cdot(w_1,w_3)+b_1$$
$$z2 = (x_1,x_2)\cdot(w_2,w_3)+b_2$$

这一步没什么好说的，将每个输入的值乘上对于的权重，输入到下一个节点，其中需要注意的是，对于每个节点，都会有$n$个输入，其中$n$为输入的总数，例如异或运算，$(1,0)$算作两个输入。

#### 激活

$$o_1 = Sigmoid(z1)$$
$$o_2 = Sigmoid(z2)$$
其中，$Sigmoid(z)$函数的定义如下

$$Sigmoid(z) = \frac{1}{1+e^{-z}}$$

那么$Sigmoid$函数的作用是什么呢，首先我们要明确，Neural Nets 能做的大部分工作为 classify，而 classify 则需要有一个 threshold，来保证最后输出的数据是处于一个是/非的状态，有着一个固定的范围。如果仅仅是使用一个分段的 0/1 函数可以做到这一点，但是导致的后果是当我们做 BACKWARD PROPAGATION 时会及其困难，因为 0/1 函数并不是一个可导的函数，于是我们采用$Sigmoid$这样连续可导的函数来作为我们的激活函数

#### 再次加权

$$z3 = (o_1,o_2)\cdot(w_5,w_6)$$

#### 再次激活

$$p = Sigmoid(z_3)$$

最后，得到的 o3 便是从两个输入中得到的预估值。

### 附：损失函数

由于 FORWARD PROPAGATION 在我们训练模型的时候会用到，我们通常会给训练的模型同时定义输入和期望值，这个时候，衡量神经网络的准确性就显得极其重要，于是我们定义如下的函数来衡量神经网络的准确率

$$loss = l = \frac{1}{n}\sum_{i=1}^{n}(d - p)^2$$
`其中，d为期望值 desire，p为预测值 prediction`

### BACKWARD PROPAGATION

如何根据这个损失函数来调整我们的模型呢？由于我们在 FORWARD PROPAGATION 里用到的函数均为连续可导的函数，我们可以根据偏导数来求出$w$变化的时候，$loss$的变化值

即，要求出以下偏导数的值

$$\frac{\partial l}{\partial w}$$

我们以 $w_5$ 为例
根据链式求导法则，我们可以得到：

$$\frac{\partial l}{\partial w_5} = \frac{\partial l}{\partial p}\cdot\frac{\partial p}{\partial z_3}\cdot\frac{\partial z_3}{\partial w_5}$$

接下来我们分别求出每个式子的值（我们设$n=1$）

$$\frac{\partial l}{\partial p} = 2\times(p-d)$$

$$\frac{\partial p}{\partial z_3} = Sigmoid'(z_3)=Sigmoid(z_3)\times(1-Sigmoid(z_3))$$

$$\frac{\partial z_3}{\partial w_5} = o_1$$

整合得：

$$\frac{\partial l}{\partial w_5} = 2 \times (p-d) \times Sigmoid(z_3) \times (1-Sigmoid(z_3)) \times o_1$$

因此我们可以从这个公式中推出，当$w_5$微量增大时，$loss$所增大的量，而我们希望得到的是$loss$损失尽量的小，于是$w_5$所需要的改变量$\delta w_5$与$\frac{\partial l}{\partial w_5}$正负性刚好相反，但由于我们需要拟合的训练模型为一个集合，因此我们需要以一种`随机梯度下降`的方法来逐渐使$loss$的值逐渐减小，最简单的做法，即定义一个$\alpha$，每次让$\delta w_5=\alpha \times \frac{\partial l}{\partial w_5}$，最后，$w_5'=w_5 - \delta w_5$，得到新的$w'_5$，

同理我们可以求出$\frac{\partial l}{\partial w_1}$、$\frac{\partial l}{\partial w_2}$、$\frac{\partial l}{\partial w_3}$、$\frac{\partial l}{\partial w_4}$

## Part II 代码

```python3
#!/usr/bin/python
# -*- coding: utf-8 -*-
import matplotlib.pyplot as plt
import numpy as np


def sigmoid(x):
    return 1 / (1 + np.exp(-x))


def deriv_sigmoid(x):
    fx = sigmoid(x)
    return fx * (1 - fx)


def loss(desire, output):
    return (desire - output) ** 2


class Neuron:
    def __init__(self, weight, bias=np.random.normal()):
        self.weight = weight
        # 偏置
        self.bias = bias

    def forward_propagation(self, inputs):
        # print("inputs: {},weight: {}".format(inputs, self.weight))
        z = np.dot(inputs, self.weight) + self.bias
        return sigmoid(z), z


class NeuralNetWork:
    def __init__(self):
        # 初始化输入值
        self.input = None
        # 初始化期望值
        self.desire = 0
        # 初始化隐藏层神经元
        self.h1 = Neuron(np.random.rand(2))
        self.h2 = Neuron(np.random.rand(2))
        # 初始化输出层神经元
        self.o = Neuron(np.random.rand(2))
        # 初始化学习率
        self.learn_rate = .5

        # 初始化各个神经元的输出（防止pycharm报错
        self.o1, self.o2, self.o3, self.z1, self.z2, self.z3 = None, None, None, None, None, None

    def forward_propagation(self, ipt, desire=None):
        self.input = ipt
        # 隐藏层运算
        self.o1, self.z1 = self.h1.forward_propagation(ipt)
        self.o2, self.z2 = self.h2.forward_propagation(ipt)
        # 输出层运算
        self.o3, self.z3 = self.o.forward_propagation(np.array([self.o1, self.o2]))
        self.desire = desire
        return self.o3

    def backward_propagation(self):
        # 计算出需要调整的输出神经元的权重值
        o1, o2, o3 = self.o1, self.o2, self.o3
        p_P_p_z3 = (o3 - self.desire) * 2
        p_w5 = deriv_sigmoid(self.z3) * o1 * p_P_p_z3
        p_w6 = deriv_sigmoid(self.z3) * o2 * p_P_p_z3
        p_b3 = deriv_sigmoid(self.z3) * p_P_p_z3
        d_w_o = np.array([p_w5, p_w6]) * self.learn_rate
        d_b_o = p_b3 * self.learn_rate
        # 计算出需要调整的隐藏层的各个神经元所需要调整的权重值
        ##  h1 ##
        p_w1 = p_P_p_z3 * deriv_sigmoid(self.z3) * self.o.weight[0] * deriv_sigmoid(self.z1) * self.input[0]
        p_w3 = p_P_p_z3 * deriv_sigmoid(self.z3) * self.o.weight[0] * deriv_sigmoid(self.z1) * self.input[1]
        p_b1 = p_P_p_z3 * deriv_sigmoid(self.z3) * self.o.weight[0] * deriv_sigmoid(self.z1)
        d_w_h1 = np.array([p_w1, p_w3]) * self.learn_rate
        d_b_h1 = p_b1 * self.learn_rate

        ## h2 ##
        p_w2 = p_P_p_z3 * deriv_sigmoid(self.z3) * self.o.weight[1] * deriv_sigmoid(self.z2) * self.input[0]
        p_w4 = p_P_p_z3 * deriv_sigmoid(self.z3) * self.o.weight[1] * deriv_sigmoid(self.z2) * self.input[1]
        p_b2 = p_P_p_z3 * deriv_sigmoid(self.z3) * self.o.weight[1] * deriv_sigmoid(self.z2)
        d_w_h2 = np.array([p_w2, p_w4]) * self.learn_rate
        d_b_h2 = p_b2 * self.learn_rate

        # 修改对应权重
        self.o.weight -= d_w_o
        self.h1.weight -= d_w_h1
        self.h2.weight -= d_w_h2
        # 修改对应的偏置
        self.o.bias -= d_b_o
        self.h1.bias -= d_b_h1
        self.h2.bias -= d_b_h2
        print("weight of o : {}, h1: {}, h2: {}; bias of o: {}, h1: {}, h2: {}".format(self.o.weight, self.h1.weight,
                                                                                       self.h2.weight, self.o.bias,
                                                                                       self.h1.bias, self.h2.bias))

    def train(self, data_set, rounds):
        loss_list = []
        cnt = 1
        for i in range(rounds):
            app = 0
            for data in data_set:
                app += loss(output=self.forward_propagation(data[0], desire=data[1]), desire=data[1])
                self.backward_propagation()
                cnt += 1
            loss_list.append(app / 4)
        return loss_list


nnw = NeuralNetWork()
rounds = 500
data_set = (
    (np.array([0, 0]), 0),
    (np.array([1, 1]), 0),
    (np.array([1, 0]), 1),
    (np.array([0, 1]), 1),
)
lst = nnw.train(data_set, rounds)

oxo = nnw.forward_propagation(data_set[0][0])
lxo = nnw.forward_propagation(data_set[1][0])
oxl = nnw.forward_propagation(data_set[2][0])
lxl = nnw.forward_propagation(data_set[3][0])
print(
    '''
after {} rounds training
result of 0 xor 0 is {}
result of 1 xor 1 is {}
result of 1 xor 0 is {}
result of 0 xor 1 is {}
'''.format(rounds * len(data_set), oxo, lxo, oxl,
           lxl))
plt.plot(list(range(0, rounds)), lst)
plt.show()
```

## Part III 总结

别的前面都提到过了，感谢为我提供帮助的人们 😁，别的以后再补充吧
