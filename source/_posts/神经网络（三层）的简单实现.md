---
title: ç¥ç»ç½‘ç»œï¼ˆä¸‰å±‚ï¼‰çš„ç®€å•å®ç°
date: 2020-03-21 09:39:16
tags: ç¥ç»ç½‘ç»œ å¼‚æˆ–è¿ç®—
keywords: ç¥ç»ç½‘ç»œ æœºå™¨å­¦ä¹  ä¸‰å±‚ å¼‚æˆ–è¿ç®—
categories: AI
description: å­¦ä¹ äº†MIT AI Courseçš„Neural Networkéƒ¨åˆ†ï¼Œå†³å®šèŠ±ç‚¹æ—¶é—´è‡ªå·±ä½¿ç”¨pythonä»é›¶æ‰“é€ ä¸€ä¸ªå±äºè‡ªå·±çš„ç®€å•ç¥ç»ç½‘ç»œ
summary_img:
---

> æœ€è¿‘è§‚çœ‹äº† MIT Open Course çš„ Neural Nets partï¼Œå¯¹äºç°ä»£äººå·¥æ™ºèƒ½å‘å±•è¿›ç¨‹é¢‡æœ‰æ„Ÿæ‚Ÿï¼Œç„¶è€Œçº¸ä¸Šå¾—æ¥ç»ˆè§‰æµ…ï¼Œå†åŠ ä¸Šä¸€ç›´å¯¹ machine learning æœ‰å…´è¶£ï¼Œæ‰€ä»¥å†³å®šä½¿ç”¨ python å®ç°ä¸€ä¸ªç®€å•çš„ç¥ç»ç½‘ç»œ

## Part I åŸç†åˆ†æ

Neural Networkï¼Œç¥ç»ç½‘ç»œï¼Œæœ¬è´¨ä¸Šæ˜¯çŸ©é˜µè¿ç®—ä»¥åŠæƒå€¼è°ƒæ•´ï¼ŒåŸºæœ¬å·¥ä½œæµç¨‹å¦‚ä¸‹ï¼š

<img src="/assets/neuralnets1.png" style="display:block;max-width:50%">

`psï¼šå›¾ä¸­å¿˜äº†åŠ biasï¼Œåé¢å…¬å¼é‡Œä¼šæœ‰è¡¥ä¸Š`

æ—¢ç„¶å·²ç»äº†è§£åˆ°äº†å·¥ä½œåŸç†ï¼Œé‚£ä¹ˆæ¥ä¸‹æ¥å°±æ˜¯æˆ‘æœ€çˆ±çš„å…¬å¼æ¨å¯¼ç¯èŠ‚ï¼ˆbushiï¼‰

### 1 FORWARD PROPAGATION (å‰æ¨)

> FORWARD PROPAGATION è¿™ä¸€æ­¥çš„ç›®çš„æ˜¯æ ¹æ®è¾“å…¥å€¼å¾—åˆ°è¾“å‡ºçš„å€¼ï¼Œä»¥æ­¤æ¥è·å–é¢„æµ‹å€¼

#### åŠ æƒ

$$z1 = (x_1,x_2)\cdot(w_1,w_3)+b_1$$
$$z2 = (x_1,x_2)\cdot(w_2,w_3)+b_2$$

è¿™ä¸€æ­¥æ²¡ä»€ä¹ˆå¥½è¯´çš„ï¼Œå°†æ¯ä¸ªè¾“å…¥çš„å€¼ä¹˜ä¸Šå¯¹äºçš„æƒé‡ï¼Œè¾“å…¥åˆ°ä¸‹ä¸€ä¸ªèŠ‚ç‚¹ï¼Œå…¶ä¸­éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œå¯¹äºæ¯ä¸ªèŠ‚ç‚¹ï¼Œéƒ½ä¼šæœ‰$n$ä¸ªè¾“å…¥ï¼Œå…¶ä¸­$n$ä¸ºè¾“å…¥çš„æ€»æ•°ï¼Œä¾‹å¦‚å¼‚æˆ–è¿ç®—ï¼Œ$(1,0)$ç®—ä½œä¸¤ä¸ªè¾“å…¥ã€‚

#### æ¿€æ´»

$$o_1 = Sigmoid(z1)$$
$$o_2 = Sigmoid(z2)$$
å…¶ä¸­ï¼Œ$Sigmoid(z)$å‡½æ•°çš„å®šä¹‰å¦‚ä¸‹

$$Sigmoid(z) = \frac{1}{1+e^{-z}}$$

é‚£ä¹ˆ$Sigmoid$å‡½æ•°çš„ä½œç”¨æ˜¯ä»€ä¹ˆå‘¢ï¼Œé¦–å…ˆæˆ‘ä»¬è¦æ˜ç¡®ï¼ŒNeural Nets èƒ½åšçš„å¤§éƒ¨åˆ†å·¥ä½œä¸º classifyï¼Œè€Œ classify åˆ™éœ€è¦æœ‰ä¸€ä¸ª thresholdï¼Œæ¥ä¿è¯æœ€åè¾“å‡ºçš„æ•°æ®æ˜¯å¤„äºä¸€ä¸ªæ˜¯/éçš„çŠ¶æ€ï¼Œæœ‰ç€ä¸€ä¸ªå›ºå®šçš„èŒƒå›´ã€‚å¦‚æœä»…ä»…æ˜¯ä½¿ç”¨ä¸€ä¸ªåˆ†æ®µçš„ 0/1 å‡½æ•°å¯ä»¥åšåˆ°è¿™ä¸€ç‚¹ï¼Œä½†æ˜¯å¯¼è‡´çš„åæœæ˜¯å½“æˆ‘ä»¬åš BACKWARD PROPAGATION æ—¶ä¼šåŠå…¶å›°éš¾ï¼Œå› ä¸º 0/1 å‡½æ•°å¹¶ä¸æ˜¯ä¸€ä¸ªå¯å¯¼çš„å‡½æ•°ï¼Œäºæ˜¯æˆ‘ä»¬é‡‡ç”¨$Sigmoid$è¿™æ ·è¿ç»­å¯å¯¼çš„å‡½æ•°æ¥ä½œä¸ºæˆ‘ä»¬çš„æ¿€æ´»å‡½æ•°

#### å†æ¬¡åŠ æƒ

$$z3 = (o_1,o_2)\cdot(w_5,w_6)$$

#### å†æ¬¡æ¿€æ´»

$$p = Sigmoid(z_3)$$

æœ€åï¼Œå¾—åˆ°çš„ o3 ä¾¿æ˜¯ä»ä¸¤ä¸ªè¾“å…¥ä¸­å¾—åˆ°çš„é¢„ä¼°å€¼ã€‚

### é™„ï¼šæŸå¤±å‡½æ•°

ç”±äº FORWARD PROPAGATION åœ¨æˆ‘ä»¬è®­ç»ƒæ¨¡å‹çš„æ—¶å€™ä¼šç”¨åˆ°ï¼Œæˆ‘ä»¬é€šå¸¸ä¼šç»™è®­ç»ƒçš„æ¨¡å‹åŒæ—¶å®šä¹‰è¾“å…¥å’ŒæœŸæœ›å€¼ï¼Œè¿™ä¸ªæ—¶å€™ï¼Œè¡¡é‡ç¥ç»ç½‘ç»œçš„å‡†ç¡®æ€§å°±æ˜¾å¾—æå…¶é‡è¦ï¼Œäºæ˜¯æˆ‘ä»¬å®šä¹‰å¦‚ä¸‹çš„å‡½æ•°æ¥è¡¡é‡ç¥ç»ç½‘ç»œçš„å‡†ç¡®ç‡

$$loss = l = \frac{1}{n}\sum_{i=1}^{n}(d - p)^2$$
`å…¶ä¸­ï¼Œdä¸ºæœŸæœ›å€¼ desireï¼Œpä¸ºé¢„æµ‹å€¼ prediction`

### BACKWARD PROPAGATION

å¦‚ä½•æ ¹æ®è¿™ä¸ªæŸå¤±å‡½æ•°æ¥è°ƒæ•´æˆ‘ä»¬çš„æ¨¡å‹å‘¢ï¼Ÿç”±äºæˆ‘ä»¬åœ¨ FORWARD PROPAGATION é‡Œç”¨åˆ°çš„å‡½æ•°å‡ä¸ºè¿ç»­å¯å¯¼çš„å‡½æ•°ï¼Œæˆ‘ä»¬å¯ä»¥æ ¹æ®åå¯¼æ•°æ¥æ±‚å‡º$w$å˜åŒ–çš„æ—¶å€™ï¼Œ$loss$çš„å˜åŒ–å€¼

å³ï¼Œè¦æ±‚å‡ºä»¥ä¸‹åå¯¼æ•°çš„å€¼

$$\frac{\partial l}{\partial w}$$

æˆ‘ä»¬ä»¥ $w_5$ ä¸ºä¾‹
æ ¹æ®é“¾å¼æ±‚å¯¼æ³•åˆ™ï¼Œæˆ‘ä»¬å¯ä»¥å¾—åˆ°ï¼š

$$\frac{\partial l}{\partial w_5} = \frac{\partial l}{\partial p}\cdot\frac{\partial p}{\partial z_3}\cdot\frac{\partial z_3}{\partial w_5}$$

æ¥ä¸‹æ¥æˆ‘ä»¬åˆ†åˆ«æ±‚å‡ºæ¯ä¸ªå¼å­çš„å€¼ï¼ˆæˆ‘ä»¬è®¾$n=1$ï¼‰

$$\frac{\partial l}{\partial p} = 2\times(p-d)$$

$$\frac{\partial p}{\partial z_3} = Sigmoid'(z_3)=Sigmoid(z_3)\times(1-Sigmoid(z_3))$$

$$\frac{\partial z_3}{\partial w_5} = o_1$$

æ•´åˆå¾—ï¼š

$$\frac{\partial l}{\partial w_5} = 2 \times (p-d) \times Sigmoid(z_3) \times (1-Sigmoid(z_3)) \times o_1$$

å› æ­¤æˆ‘ä»¬å¯ä»¥ä»è¿™ä¸ªå…¬å¼ä¸­æ¨å‡ºï¼Œå½“$w_5$å¾®é‡å¢å¤§æ—¶ï¼Œ$loss$æ‰€å¢å¤§çš„é‡ï¼Œè€Œæˆ‘ä»¬å¸Œæœ›å¾—åˆ°çš„æ˜¯$loss$æŸå¤±å°½é‡çš„å°ï¼Œäºæ˜¯$w_5$æ‰€éœ€è¦çš„æ”¹å˜é‡$\delta w_5$ä¸$\frac{\partial l}{\partial w_5}$æ­£è´Ÿæ€§åˆšå¥½ç›¸åï¼Œä½†ç”±äºæˆ‘ä»¬éœ€è¦æ‹Ÿåˆçš„è®­ç»ƒæ¨¡å‹ä¸ºä¸€ä¸ªé›†åˆï¼Œå› æ­¤æˆ‘ä»¬éœ€è¦ä»¥ä¸€ç§`éšæœºæ¢¯åº¦ä¸‹é™`çš„æ–¹æ³•æ¥é€æ¸ä½¿$loss$çš„å€¼é€æ¸å‡å°ï¼Œæœ€ç®€å•çš„åšæ³•ï¼Œå³å®šä¹‰ä¸€ä¸ª$\alpha$ï¼Œæ¯æ¬¡è®©$\delta w_5=\alpha \times \frac{\partial l}{\partial w_5}$ï¼Œæœ€åï¼Œ$w_5'=w_5 - \delta w_5$ï¼Œå¾—åˆ°æ–°çš„$w'_5$ï¼Œ

åŒç†æˆ‘ä»¬å¯ä»¥æ±‚å‡º$\frac{\partial l}{\partial w_1}$ã€$\frac{\partial l}{\partial w_2}$ã€$\frac{\partial l}{\partial w_3}$ã€$\frac{\partial l}{\partial w_4}$

## Part II ä»£ç 

```python3
#!/usr/bin/python
# -*- coding: utf-8 -*-
import matplotlib.pyplot as plt
import numpy as np


def sigmoid(x):
    return 1 / (1 + np.exp(-x))


def deriv_sigmoid(x):
    fx = sigmoid(x)
    return fx * (1 - fx)


def loss(desire, output):
    return (desire - output) ** 2


class Neuron:
    def __init__(self, weight, bias=np.random.normal()):
        self.weight = weight
        # åç½®
        self.bias = bias

    def forward_propagation(self, inputs):
        # print("inputs: {},weight: {}".format(inputs, self.weight))
        z = np.dot(inputs, self.weight) + self.bias
        return sigmoid(z), z


class NeuralNetWork:
    def __init__(self):
        # åˆå§‹åŒ–è¾“å…¥å€¼
        self.input = None
        # åˆå§‹åŒ–æœŸæœ›å€¼
        self.desire = 0
        # åˆå§‹åŒ–éšè—å±‚ç¥ç»å…ƒ
        self.h1 = Neuron(np.random.rand(2))
        self.h2 = Neuron(np.random.rand(2))
        # åˆå§‹åŒ–è¾“å‡ºå±‚ç¥ç»å…ƒ
        self.o = Neuron(np.random.rand(2))
        # åˆå§‹åŒ–å­¦ä¹ ç‡
        self.learn_rate = .5

        # åˆå§‹åŒ–å„ä¸ªç¥ç»å…ƒçš„è¾“å‡ºï¼ˆé˜²æ­¢pycharmæŠ¥é”™
        self.o1, self.o2, self.o3, self.z1, self.z2, self.z3 = None, None, None, None, None, None

    def forward_propagation(self, ipt, desire=None):
        self.input = ipt
        # éšè—å±‚è¿ç®—
        self.o1, self.z1 = self.h1.forward_propagation(ipt)
        self.o2, self.z2 = self.h2.forward_propagation(ipt)
        # è¾“å‡ºå±‚è¿ç®—
        self.o3, self.z3 = self.o.forward_propagation(np.array([self.o1, self.o2]))
        self.desire = desire
        return self.o3

    def backward_propagation(self):
        # è®¡ç®—å‡ºéœ€è¦è°ƒæ•´çš„è¾“å‡ºç¥ç»å…ƒçš„æƒé‡å€¼
        o1, o2, o3 = self.o1, self.o2, self.o3
        p_P_p_z3 = (o3 - self.desire) * 2
        p_w5 = deriv_sigmoid(self.z3) * o1 * p_P_p_z3
        p_w6 = deriv_sigmoid(self.z3) * o2 * p_P_p_z3
        p_b3 = deriv_sigmoid(self.z3) * p_P_p_z3
        d_w_o = np.array([p_w5, p_w6]) * self.learn_rate
        d_b_o = p_b3 * self.learn_rate
        # è®¡ç®—å‡ºéœ€è¦è°ƒæ•´çš„éšè—å±‚çš„å„ä¸ªç¥ç»å…ƒæ‰€éœ€è¦è°ƒæ•´çš„æƒé‡å€¼
        ##  h1 ##
        p_w1 = p_P_p_z3 * deriv_sigmoid(self.z3) * self.o.weight[0] * deriv_sigmoid(self.z1) * self.input[0]
        p_w3 = p_P_p_z3 * deriv_sigmoid(self.z3) * self.o.weight[0] * deriv_sigmoid(self.z1) * self.input[1]
        p_b1 = p_P_p_z3 * deriv_sigmoid(self.z3) * self.o.weight[0] * deriv_sigmoid(self.z1)
        d_w_h1 = np.array([p_w1, p_w3]) * self.learn_rate
        d_b_h1 = p_b1 * self.learn_rate

        ## h2 ##
        p_w2 = p_P_p_z3 * deriv_sigmoid(self.z3) * self.o.weight[1] * deriv_sigmoid(self.z2) * self.input[0]
        p_w4 = p_P_p_z3 * deriv_sigmoid(self.z3) * self.o.weight[1] * deriv_sigmoid(self.z2) * self.input[1]
        p_b2 = p_P_p_z3 * deriv_sigmoid(self.z3) * self.o.weight[1] * deriv_sigmoid(self.z2)
        d_w_h2 = np.array([p_w2, p_w4]) * self.learn_rate
        d_b_h2 = p_b2 * self.learn_rate

        # ä¿®æ”¹å¯¹åº”æƒé‡
        self.o.weight -= d_w_o
        self.h1.weight -= d_w_h1
        self.h2.weight -= d_w_h2
        # ä¿®æ”¹å¯¹åº”çš„åç½®
        self.o.bias -= d_b_o
        self.h1.bias -= d_b_h1
        self.h2.bias -= d_b_h2
        print("weight of o : {}, h1: {}, h2: {}; bias of o: {}, h1: {}, h2: {}".format(self.o.weight, self.h1.weight,
                                                                                       self.h2.weight, self.o.bias,
                                                                                       self.h1.bias, self.h2.bias))

    def train(self, data_set, rounds):
        loss_list = []
        cnt = 1
        for i in range(rounds):
            app = 0
            for data in data_set:
                app += loss(output=self.forward_propagation(data[0], desire=data[1]), desire=data[1])
                self.backward_propagation()
                cnt += 1
            loss_list.append(app / 4)
        return loss_list


nnw = NeuralNetWork()
rounds = 500
data_set = (
    (np.array([0, 0]), 0),
    (np.array([1, 1]), 0),
    (np.array([1, 0]), 1),
    (np.array([0, 1]), 1),
)
lst = nnw.train(data_set, rounds)

oxo = nnw.forward_propagation(data_set[0][0])
lxo = nnw.forward_propagation(data_set[1][0])
oxl = nnw.forward_propagation(data_set[2][0])
lxl = nnw.forward_propagation(data_set[3][0])
print(
    '''
after {} rounds training
result of 0 xor 0 is {}
result of 1 xor 1 is {}
result of 1 xor 0 is {}
result of 0 xor 1 is {}
'''.format(rounds * len(data_set), oxo, lxo, oxl,
           lxl))
plt.plot(list(range(0, rounds)), lst)
plt.show()
```

## Part III æ€»ç»“

åˆ«çš„å‰é¢éƒ½æåˆ°è¿‡äº†ï¼Œæ„Ÿè°¢ä¸ºæˆ‘æä¾›å¸®åŠ©çš„äººä»¬ ğŸ˜ï¼Œåˆ«çš„ä»¥åå†è¡¥å……å§
